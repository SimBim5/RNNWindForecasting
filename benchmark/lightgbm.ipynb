{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snsa\n",
    "import ipywidgets as widgets\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load Training Data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/cache/wind_farm_data/wind_data_train_seq24_pred3_num5_normrobust_minmax_normalize_modetrain_clusternearest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m loaded_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_sequences_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m loaded_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\SimBim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SimBim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\SimBim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:345\u001b[0m, in \u001b[0;36mBaseEstimator.__setstate__\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msklearn.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    346\u001b[0m         pickle_version \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_sklearn_version\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre-0.18\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pickle_version \u001b[38;5;241m!=\u001b[39m __version__:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Training Data\n",
    "data_path = \"../data/cache/wind_farm_data/wind_data_train_seq24_pred3_num5_normrobust_minmax_normalize_modetrain_clusternearest.pt\"\n",
    "loaded_data = torch.load(data_path)\n",
    "input_sequences = loaded_data['input_sequences_tensor'].numpy()\n",
    "ground_truth = loaded_data['ground_truth_tensor'].numpy()\n",
    "input_sequences = input_sequences[:, :-3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_data_path = \"../data/cache/wind_farm_data/wind_data_test_seq24_pred3_num5_normrobust_minmax_normalize_modetest_clusternearest.pt\"\n",
    "loaded_test_data = torch.load(test_data_path)\n",
    "input_sequences_test = loaded_test_data['input_sequences_tensor'].numpy()\n",
    "ground_truth_test = loaded_test_data['ground_truth_tensor'].numpy()\n",
    "input_sequences_test = input_sequences_test[:, :-3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SimBim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.4.1.post1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\SimBim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.4.1.post1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Weather Data for Training\n",
    "weather_data_path = \"../data/cache/weather/wind_data_train_seq24_pred3_num5_normrobust_minmax_normalize_modetrain_clusternearest_temp2m_rh2m_wind100m_winddir100m.pt\"\n",
    "loaded_weather_train = torch.load(weather_data_path)\n",
    "weather_features_train = loaded_weather_train['weather_data_tensor'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Weather Data for Test\n",
    "weather_data_test_path = \"../data/cache/weather/wind_data_test_seq24_pred3_num5_normrobust_minmax_normalize_modetest_clusternearest_temp2m_rh2m_wind100m_winddir100m.pt\"\n",
    "loaded_weather_test = torch.load(weather_data_test_path)\n",
    "weather_features_test = loaded_weather_test['weather_data_tensor'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spatial Data for Training\n",
    "spatial_data_path = \"../data/cache/spatial/train_seq24_pred3_num5_clusternearest.pt\"\n",
    "spatial_data = torch.load(spatial_data_path)\n",
    "correlation = spatial_data['correlation'].numpy()\n",
    "distance = spatial_data['distance'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spatial Data for Test\n",
    "spatial_data_test_path = \"../data/cache/spatial/test_seq24_pred3_num5_clusternearest.pt\"\n",
    "spatial_data_test = torch.load(spatial_data_test_path)\n",
    "correlation_test = spatial_data_test['correlation'].numpy()\n",
    "distance_test = spatial_data_test['distance'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minput_sequences\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ground_truth\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_sequences_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "print(input_sequences.shape)\n",
    "print(ground_truth.shape)\n",
    "print(input_sequences_test.shape)\n",
    "print(ground_truth_test.shape)\n",
    "print(weather_features_train.shape)\n",
    "print(weather_features_test.shape)\n",
    "print(correlation.shape)\n",
    "print(distance.shape)\n",
    "print(correlation_test.shape)\n",
    "print(distance_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the weather data for training and testing\n",
    "weather_features_train_flat = weather_features_train.reshape(weather_features_train.shape[0], -1)\n",
    "weather_features_test_flat = weather_features_test.reshape(weather_features_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training and Testing Data\n",
    "X_flat = input_sequences.reshape(input_sequences.shape[0], -1)\n",
    "X_test_flat = input_sequences_test.reshape(input_sequences_test.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_flat = ground_truth.reshape(ground_truth.shape[0], -1)\n",
    "y_test_flat = ground_truth_test.reshape(ground_truth_test.shape[0], -1)\n",
    "y_train_flat = np.array(y_train_flat)\n",
    "y_test_flat = np.array(y_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192401, 540)\n",
      "(66410, 540)\n",
      "(192401, 120)\n",
      "(66410, 120)\n",
      "(192401, 10)\n",
      "(192401, 10)\n",
      "(66410, 10)\n",
      "(66410, 10)\n"
     ]
    }
   ],
   "source": [
    "print(weather_features_train_flat.shape)\n",
    "print(weather_features_test_flat.shape)\n",
    "print(X_flat.shape)\n",
    "print(X_test_flat.shape)\n",
    "print(correlation.shape)\n",
    "print(distance.shape)\n",
    "print(correlation_test.shape)\n",
    "print(distance_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_augmented_train = np.concatenate([X_flat, weather_features_train_flat, correlation, distance], axis=1)\n",
    "X_augmented_test = np.concatenate([X_test_flat, weather_features_test_flat, correlation_test, distance_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_augmented_train, label=y_train_flat)\n",
    "test_data = lgb.Dataset(X_augmented_test, label=y_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model parameters\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": {\"l2\", \"l1\"},\n",
    "    \"num_leaves\": 128,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_augmented_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mlgb_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiOutputRegressor(grid_search)\n\u001b[1;32m---> 22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_augmented_train\u001b[49m, y_train_flat)\n\u001b[0;32m     23\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     24\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_augmented_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Define your model\n",
    "lgb_model = lgb.LGBMRegressor()\n",
    "\n",
    "# Setup a parameter grid to explore\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'n_estimators': [100, 200],\n",
    "    'num_leaves': [31, 51],\n",
    "    'feature_fraction': [0.8, 0.9],\n",
    "    'bagging_fraction': [0.7, 0.9],\n",
    "    'bagging_freq': [5, 9]\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, cv=2, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "model = MultiOutputRegressor(grid_search)\n",
    "\n",
    "model.fit(X_augmented_train, y_train_flat)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Grid search took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "best_params = model.estimator.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "y_pred = model.predict(X_augmented_test)\n",
    "\n",
    "mse_values = [mean_squared_error(y_test_flat[i], y_pred[i]) for i in range(3)]\n",
    "mae_values = [mean_absolute_error(y_test_flat[i], y_pred[i]) for i in range(3)]\n",
    "\n",
    "overall_mse = np.mean(mse_values)\n",
    "overall_mae = np.mean(mae_values)\n",
    "\n",
    "error_metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'MAE'],\n",
    "    'Target 1': [mse_values[0], mae_values[0]],\n",
    "    'Target 2': [mse_values[1], mae_values[1]],\n",
    "    'Target 3': [mse_values[2], mae_values[2]],\n",
    "    'Overall': [overall_mse, overall_mae]\n",
    "})\n",
    "print(error_metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.356277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.498758\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302086 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.502442\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.501875\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.376080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.504025\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.503769\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.444159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.493596\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.378222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.498043\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.318109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.496929\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.498913\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.501525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.499248\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.392408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.487843\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.383850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.492621\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.371545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.491138\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.350994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.492818\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.337881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173385\n",
      "[LightGBM] [Info] Number of data points in the train set: 192401, number of used features: 680\n",
      "[LightGBM] [Info] Start training from score 0.493865\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=LGBMRegressor(learning_rate=0.05))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=LGBMRegressor(learning_rate=0.05))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(learning_rate=0.05)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(learning_rate=0.05)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputRegressor(estimator=LGBMRegressor(learning_rate=0.05))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMRegressor(learning_rate=0.05, n_estimators=100)\n",
    "model = MultiOutputRegressor(lgb_model)\n",
    "model.fit(X_augmented_train, y_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_augmented_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Metric  Target 1  Target 2  Target 3   Overall\n",
      "0    MSE  0.060854  0.004654  0.039385  0.034964\n",
      "1   RMSE  0.246686  0.068221  0.198458  0.171121\n",
      "2    MAE  0.189209  0.050974  0.140561  0.126915\n",
      "3    MBE  0.180252  0.007922  0.139162  0.109112\n",
      "4  MAAPE  0.581898  0.182977  0.762676  0.509184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SimBim\\AppData\\Local\\Temp\\ipykernel_14024\\239276761.py:12: RuntimeWarning: divide by zero encountered in divide\n",
      "  maape = np.mean(np.arctan(np.abs((y_test_flat[i] - y_pred[i]) / y_test_flat[i])))\n",
      "C:\\Users\\SimBim\\AppData\\Local\\Temp\\ipykernel_14024\\239276761.py:12: RuntimeWarning: divide by zero encountered in divide\n",
      "  maape = np.mean(np.arctan(np.abs((y_test_flat[i] - y_pred[i]) / y_test_flat[i])))\n"
     ]
    }
   ],
   "source": [
    "mse_values = []\n",
    "mae_values = []\n",
    "rmse_values = []\n",
    "mbe_values = []\n",
    "maape_values = []\n",
    "\n",
    "for i in range(3):\n",
    "    mse = mean_squared_error(y_test_flat[i], y_pred[i])\n",
    "    mae = mean_absolute_error(y_test_flat[i], y_pred[i])\n",
    "    rmse = np.sqrt(mse)\n",
    "    mbe = np.mean(y_pred[i] - y_test_flat[i])\n",
    "    maape = np.mean(np.arctan(np.abs((y_test_flat[i] - y_pred[i]) / y_test_flat[i])))\n",
    "        \n",
    "    mse_values.append(mse)\n",
    "    mae_values.append(mae)\n",
    "    rmse_values.append(rmse)\n",
    "    mbe_values.append(mbe)\n",
    "    maape_values.append(maape)\n",
    "\n",
    "overall_mse = np.mean(mse_values)\n",
    "overall_mae = np.mean(mae_values)\n",
    "overall_rmse = np.mean(rmse_values)\n",
    "overall_mbe = np.mean(mbe_values)\n",
    "overall_maape = np.mean(maape_values)\n",
    "\n",
    "error_metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'MBE', 'MAAPE'],\n",
    "    'Target 1': [mse_values[0], rmse_values[0], mae_values[0], mbe_values[0], maape_values[0]],\n",
    "    'Target 2': [mse_values[1], rmse_values[1], mae_values[1], mbe_values[1], maape_values[1]],\n",
    "    'Target 3': [mse_values[2], rmse_values[2], mae_values[2], mbe_values[2], maape_values[2]],\n",
    "    'Overall': [overall_mse, overall_rmse, overall_mae, overall_mbe, overall_maape]\n",
    "})\n",
    "\n",
    "print(error_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb33a2879bf74de1b0bf00d19ed1f85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Example Index:', max=66409, style=SliderStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c791a1830c274add8c51e752e03a4c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Button, HBox, VBox\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"mako\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "y_pred_reshaped = y_pred.reshape(-1, 3, 5)\n",
    "y_test_reshaped = y_test_flat.reshape(-1, 3, 5)\n",
    "\n",
    "# Widget setup\n",
    "example_index_slider = widgets.IntSlider(value=0, min=0, max=len(y_test_reshaped)-1, step=1, description='Example Index:', readout=True, style={'description_width': 'initial'})\n",
    "output_plot = widgets.Output()\n",
    "\n",
    "def plot_results(example_index):\n",
    "    output_plot.clear_output()\n",
    "    with output_plot:\n",
    "        for feature in range(5):  # Assuming 5 features as per your setup\n",
    "            fig, ax = plt.subplots(figsize=(17, 1.5))\n",
    "            historical_series = input_sequences[example_index, :, feature]  # Last 24 historical points\n",
    "\n",
    "            # Plotting historical data\n",
    "            x_historical_series = list(range(1, 25))\n",
    "            sns.lineplot(x=x_historical_series, y=historical_series, marker='o', dashes=False, color='#165DB1', ax=ax)\n",
    "\n",
    "            # Concatenating the last historical point for continuity in the plot\n",
    "            full_ground_truth_series = np.concatenate([historical_series[-1:], y_test_reshaped[example_index, :, feature]])\n",
    "            full_prediction_series = np.concatenate([historical_series[-1:], y_pred_reshaped[example_index, :, feature]])\n",
    "\n",
    "            # Extended x-axis for future predictions\n",
    "            x_extended_series = list(range(24, 28))\n",
    "\n",
    "            # Plotting actual vs. predicted values\n",
    "            sns.lineplot(x=x_extended_series, y=full_ground_truth_series, marker='o', dashes=True, color='#165DB1', ax=ax)\n",
    "            sns.lineplot(x=x_extended_series, y=full_prediction_series, marker='o', dashes=True, color='#C680BB', ax=ax)\n",
    "\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Calculating and displaying error metrics for each feature\n",
    "            mae = mean_absolute_error(y_test_reshaped[example_index, :, feature], y_pred_reshaped[example_index, :, feature])\n",
    "            mse = mean_squared_error(y_test_reshaped[example_index, :, feature], y_pred_reshaped[example_index, :, feature])\n",
    "\n",
    "            metrics_text = f\"Feature {feature+1}\\nMAE: {mae:.5f}\\nMSE: {mse:.5f}\"\n",
    "            ax.text(1.05, 0.5, metrics_text, transform=ax.transAxes, fontsize=15, verticalalignment='center', bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=1'))\n",
    "\n",
    "            sns.despine()\n",
    "            plt.show()\n",
    "\n",
    "def example_index_changed(change):\n",
    "    plot_results(change['new'])\n",
    "\n",
    "example_index_slider.observe(example_index_changed, names='value')\n",
    "\n",
    "display(example_index_slider, output_plot)\n",
    "plot_results(example_index_slider.value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
